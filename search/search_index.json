{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Primer parcial: Inteligencia Artificial","text":"<p>Alumno: Hern\u00e1ndez Astorga Francisco Ricardo</p> <p>Expediente: 223219591</p> <p>Semestre: 6\u00b0</p> <p>Repositorio de notas personales para la materia de Inteligencia Artificial.</p>"},{"location":"#clase-12012026","title":"Clase \u2013 12/01/2026","text":"<p>\u00bfQu\u00e9 es la Inteligencia Artificial?</p> <p>Es un sistema/herramienta que imita la inteligencia y razonamiento humano para llevar a cabo la resolucion de problemas. Un punto interesante es cuestionar si es correcto que la IA imite \u00fanicamente la inteligencia humana.</p> <p>\"Maximizar la esperanza (expectation) de una utilidad futura\".</p>"},{"location":"#clase-13012026","title":"Clase \u2013 13/01/2026","text":"<p>PEAS (Performance, Entorno, Actuadores, Sensores)</p> <p>PEAS es un marco de referencia para describir una tarea desde el punto de vista de un agente inteligente.</p> <p>Permite identificar: - Performance (Desempe\u00f1o): qu\u00e9 tan bien realiza la tarea el agente. - Environment (Entorno): el mundo en el que opera el agente. - Actuators (Actuadores): c\u00f3mo el agente act\u00faa sobre el entorno. - Sensors (Sensores): c\u00f3mo el agente percibe el entorno.</p> <p>Durante la clase nos enfocamos principalmente en la parte de sensores, analizando c\u00f3mo la informaci\u00f3n disponible condiciona las decisiones del agente.</p> <p>Ejemplo 1: Torres de Hanoi</p> <p>En este ejemplo, analizamos los posibles estados que tiene el juego de torres de Hanoi.  </p> <p> </p> <p>Donde como se ve en la imagen, se tienen 5 discos, uno cada vez mas grande que el anterior y deben colocarse en otra de las torres siguiendo las condiciones y esto nos queda de la siguiente manera: S = [S\u2081, S\u2082, S\u2083, S\u2084, S\u2085] S\u1d62 \u2208 {A, B, C} |S| = 3\u2075</p> <p>Ejemplo 2: 3 esclavistas y 3 trabajadores sin paga cruzando un r\u00edo</p> <p>En este problema se ve reflejado que aunque haya menos combinaciones, es mas complejo debido a las rutas posibles.  </p> <p> </p> <p>Aqui los estados serian el numero de esclavistas de un lado del rio, el numero de trabajadores sin paga de un lado del rio y el lado del rio, dandonos la operacion de 4 x 4 x 2, dando un total de <code>32</code> estados.</p>"},{"location":"#clase-14012026","title":"Clase \u2013 14/01/2026","text":"<p>Propiedades del entorno</p> <ul> <li>Est\u00e1tico: El entorno no cambia si el agente no act\u00faa. Es como resolver un crucigrama; el papel no va a cambiar de lugar ni las letras se van a mover mientras decides qu\u00e9 escribir.  </li> <li>Din\u00e1mico: El entorno cambia constantemente, incluso si el agente se queda quieto. Como jugar al f\u00fatbol o conducir un coche: si te detienes a pensar demasiado, la situaci\u00f3n a tu alrededor ya es otra.  </li> <li>Discreto: Hay un n\u00famero finito y bien definido de posibilidades. En el ajedrez, las casillas est\u00e1n numeradas (A1, B2) y los turnos son claros. No hay un \"punto medio\" entre la casilla A1 y la A2.  </li> <li>Continuo: Las variables fluyen sin saltos, como el tiempo, la velocidad o la posici\u00f3n exacta. Un robot que camina opera en un entorno continuo porque su \u00e1ngulo de pierna puede ser <code>30\u00b0</code>, <code>30.1\u00b0</code>, <code>30.001\u00b0</code>, etc.  </li> <li>Totalmente Observable: El agente tiene acceso a toda la informaci\u00f3n necesaria para tomar una decisi\u00f3n. Ejemplo: El ajedrez (ves todo el tablero).  </li> <li>Parcialmente Observable: El agente solo ve una parte. Ejemplo: El p\u00f3ker (no ves las cartas de los dem\u00e1s) o conducir (no sabes qu\u00e9 hay a la vuelta de la esquina).  </li> <li>No Observable: El agente no tiene sensores o no puede percibir nada del entorno. B\u00e1sicamente, act\u00faa a ciegas.  </li> <li>Determinista: Si haces una acci\u00f3n, el resultado siempre ser\u00e1 el mismo. Si en un videojuego mueves una pieza a la derecha, siempre termina a la derecha. No hay azar.  </li> <li>Estoc\u00e1stico: Hay incertidumbre o azar. Aunque t\u00fa hagas lo mismo, el resultado puede variar. Ejemplo: Tirar un dado o el clima; hay una probabilidad, pero no una certeza absoluta.</li> <li>Epis\u00f3dico: Cada acci\u00f3n es independiente. Como una IA que clasifica fotos: si clasifica mal una foto de un gato, eso no afecta en nada su capacidad para clasificar la siguiente foto de un perro.</li> <li>Secuencial: Lo que hagas ahora afectar\u00e1 lo que pase despu\u00e9s. En un juego de estrategia, si pierdes a tu mejor unidad al principio, esa decisi\u00f3n \"te persigue\" durante el resto de la partida.</li> </ul> <p>Propiedades del Entorno y Funciones de Transici\u00f3n</p> <p>Tambien vimos ejemplos de como se representarian algunas propiedades de entorno, ejemplos como los siguientes:</p> <p>1. Entorno Est\u00e1tico, Determinista, Observable, Discreto. F\u00f3rmula: \\(S = f(a)\\) El estado final depende \u00fanica y exclusivamente de la acci\u00f3n realizada. El entorno no cambia por s\u00ed solo y no importa el historial previo. Un ejemplo podria ser un conversor de unidades, como podria ser de Celsius a Fahrenheit. Si la acci\u00f3n es ingresar <code>100</code>, el estado de salida siempre ser\u00e1 <code>212</code>. No importa qu\u00e9 n\u00famero convertiste antes ni cu\u00e1nto tiempo esperes.</p> <p>2. Entorno Din\u00e1mico, Determinista, Observable, Discreto. F\u00f3rmula: \\(S_{k+1} = f(S_k, a_k)\\) El entorno evoluciona. El siguiente estado (\\(S_{k+1}\\)) es el resultado de aplicar una acci\u00f3n (\\(a_k\\)) sobre el estado actual (\\(S_k\\)). Aqu\u00ed el \"pasado\" inmediato o la situaci\u00f3n actual es cr\u00edtica. En el ajedrez, para saber c\u00f3mo quedar\u00e1 el tablero (nuevo estado), necesitas saber c\u00f3mo estaban las piezas antes (estado actual) y qu\u00e9 movimiento hiciste (acci\u00f3n).</p> <p>3. Entorno Estoc\u00e1stico, Estatico, Discreto) F\u00f3rmula: \\(S = (S^1, ... , S^m)\\) donde \\(m\\) es la cardinalidad de \\(S\\). \\(Pr[S | a]\\) = \\([Pr[S = s^1 | a], ... , Pr[S = s^m | a]]\\) Esto es cuando no hay certeza absoluta. La misma acci\u00f3n en el mismo estado puede llevar a resultados diferentes debido al azar o a variables desconocidas. Un ejemplo podria ser lanzar un dado. El estado inicial es el dado en tu mano (\\(S\\)), la acci\u00f3n es lanzar (\\(a\\)), pero el estado final es incierto, tienes una probabilidad de \\(1/6\\) para cada cara. Aunque el entorno sea est\u00e1tico (el dado no cambia de cara mientras piensas), el resultado de la acci\u00f3n es aleatorio.</p>"},{"location":"#clase-15012026","title":"Clase \u2013 15/01/2026","text":"<p>En la clase de hoy profundizamos en la estructura de los agentes y c\u00f3mo interact\u00faan con su entorno.</p> <p>\u00bfQu\u00e9 es un Agente? Un agente es cualquier entidad que percibe su entorno a trav\u00e9s de sensores y act\u00faa sobre \u00e9l mediante actuadores.</p> <ul> <li>Percepciones: Se refiere al contenido que los sensores del agente est\u00e1n recibiendo en un momento dado.</li> <li>Funci\u00f3n del Agente: Es la descripci\u00f3n matem\u00e1tica que mapea cualquier secuencia de percepciones con una acci\u00f3n (\\(act = AgentFn(percept)\\)).</li> <li>Programa del Agente: Es la implementaci\u00f3n real y f\u00edsica de la funci\u00f3n del agente que corre sobre una arquitectura espec\u00edfica.</li> </ul> <p>Ejemplo: El Mundo de la Aspiradora Analizamos un modelo simplificado donde un robot opera en un entorno con dos localizaciones, A y B.</p> <ul> <li>Percepciones: El agente percibe su ubicaci\u00f3n actual y si hay suciedad (ej. <code>[A, Sucio]</code>).</li> <li>Acciones: El agente puede decidir entre <code>Izquierda</code>, <code>Derecha</code>, <code>Succionar</code> o <code>NoOp</code> (no hacer nada).</li> </ul> <p>Racionalidad y Desempe\u00f1o Un agente inteligente no es necesariamente perfecto u omnisciente, sino que busca ser racional, como hablamos en clases anteriores.</p> <ul> <li>Medida de desempe\u00f1o: Es el criterio objetivo que eval\u00faa qu\u00e9 tan exitosa es una secuencia de estados en el entorno (ej. ganar puntos por cada cuadro limpio).</li> <li>Agente Racional: Es aquel que elige la acci\u00f3n que maximiza el valor esperado de su medida de desempe\u00f1o, bas\u00e1ndose en su secuencia de percepciones y su conocimiento previo.</li> <li>Diferencia clave: La racionalidad no es igual a la perfecci\u00f3n, un agente racional puede fallar si la informaci\u00f3n es incompleta, pero su decisi\u00f3n sigue siendo la mejor posible con los datos que ten\u00eda.</li> </ul> <p>Tipos de Agentes Dependiendo de la complejidad de su \"cerebro\", clasificamos a los agentes en:</p> <ol> <li>Agentes de Reflejo Simple: Toma decisiones bas\u00e1ndose \u00fanicamente en lo que percibe en este preciso instante (\"Si veo X, hago Y\"). Ignora todo lo que pas\u00f3 antes.     \\(a = f(p)\\)</li> <li>Agentes basados en el historial: Toma decisiones considerando toda la secuencia de cosas que ha percibido desde que se encendi\u00f3, no solo la actual.    \\(a = f([p_1, p_2, \\dots, p_n])\\)</li> <li>Agentes basados en modelos: Mantiene un \"estado interno\" (una memoria) para rastrear aspectos del mundo que no puede ver en este momento. Sabe c\u00f3mo cambia el mundo por s\u00ed solo y c\u00f3mo lo afectan sus acciones.    \\(a = f_m(p)\\)</li> <li>Agentes basados en metas: No solo reacciona, sino que planea. Tiene un objetivo claro (meta) y busca la secuencia de acciones necesaria para alcanzarlo.    \\(a = f_m(p, s)\\)</li> <li>Agentes basados en utilidad: Va un paso m\u00e1s all\u00e1 de las metas. No solo quiere llegar al objetivo, sino hacerlo de la mejor manera posible (m\u00e1s r\u00e1pido, m\u00e1s seguro, m\u00e1s barato). Maximiza una medida de \"felicidad\" o utilidad.    \\(a = f_m(p, s)\\)</li> </ol>"},{"location":"#clase-16012026","title":"Clase \u2013 16/01/2026","text":"<p>En esta clase nos enfocamos mas en como trasncurriria el curso, viendo el siguiente cronograma:</p> <p></p> <p>Tabien vimos los distintos tipos de agentes que veriamos en el curso:</p> Agentes reactivos Agentes basados en metas Modelos basados en utilidad Estocasticos, Discretos, Observable, Deterministas Dinamico, Discretos, Observable, Deterministas Estocasticos <p>Y por ultimo vimos los criterios de evalucion:</p> <p>Actividades de evaluaci\u00f3n continua (notas): 10%  </p> <p>Actividades de evaluaci\u00f3n de conocimientos (problemarios): 30%  </p> <p>Actividades de evaluaci\u00f3n de competencias (laboratorios): 30%  </p> <p>Ex\u00e1menes: 30%</p> <p>Donde se menciono que se necesita el 60% minimo, en las actividades de evalucion de competencias para aprobar el curso.</p>"},{"location":"#clase-19012026","title":"Clase \u2013 19/01/2026","text":"<p>En esta clase profundizamos mas sobre el entorno, donde vimos que:</p> <p>\\(S = Estado\\) \\(S = (S_1, . . ., S_n) \\in D_1 x . . x D_n = S\\) \\(a_t \\in A = (a_1, . . ., a_m)\\) \\(a_t \\in A(s_t)\\) acciones legales \\(p_t \\in \\rho\\) espacio de percepciones \\(p_t = percepci\u00f3n(S_t)\\) \\(percepci\u00f3n: S \\rightarrow \\rho\\) \\(S_{t + 1} = transici\u00f3n(S_t, a_t)\\) \\(c = costo(s_t, a_t, S_{t + 1})\\)</p> <p>Y analizamos y discutimos el codigo <code>doscuartos_o.py</code> y <code>doscuartos_f.py</code>. </p> <pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\ndoscuartos_f.py\n----------------\n\nEjemplo de un entorno muy simple y agentes idem\n\n\"\"\"\n\nimport entornos_f\nfrom random import choice\n\n\n__author__ = 'juliowaissman'\n\n\nclass DosCuartos(entornos_f.Entorno):\n    \"\"\"\n    Clase para un entorno de dos cuartos.\n    Muy sencilla solo regrupa m\u00e9todos.\n\n    El estado se define como (robot, A, B)\n    donde robot puede tener los valores \"A\", \"B\"\n    A y B pueden tener los valores \"limpio\", \"sucio\"\n\n    Las acciones v\u00e1lidas en el entorno son\n        (\"ir_A\", \"ir_B\", \"limpiar\", \"nada\").\n\n    Todas las acciones son v\u00e1lidas en todos los estados.\n\n    Los sensores es una tupla (robot, limpio?)\n    con la ubicaci\u00f3n del robot y el estado de limpieza\n\n    \"\"\"\n    def accion_legal(self, _, accion):\n        return accion in (\"ir_A\", \"ir_B\", \"limpiar\", \"nada\")\n\n    def transicion(self, estado, acci\u00f3n):\n        robot, a, b = estado\n\n        c_local = 0 if a == b == \"limpio\" and acci\u00f3n == \"nada\" else 1\n\n        return ((estado, c_local) if a == \"nada\" else\n                ((\"A\", a, b), c_local) if acci\u00f3n == \"ir_A\" else\n                ((\"B\", a, b), c_local) if acci\u00f3n == \"ir_B\" else\n                ((robot, \"limpio\", b), c_local) if robot == \"A\" else\n                ((robot, a, \"limpio\"), c_local))\n\n    def percepcion(self, estado):\n        return estado[0], estado[\" AB\".find(estado[0])]\n\n\nclass AgenteAleatorio(entornos_f.Agente):\n    \"\"\"\n    Un agente que solo regresa una accion al azar entre las acciones legales\n\n    \"\"\"\n    def __init__(self, acciones):\n        self.acciones = acciones\n\n    def programa(self, _):\n        return choice(self.acciones)\n\n\nclass AgenteReactivoDoscuartos(entornos_f.Agente):\n    \"\"\"\n    Un agente reactivo simple\n\n    \"\"\"\n    def programa(self, percepcion):\n        robot, situacion = percepcion\n        return ('limpiar' if situacion == 'sucio' else\n                'ir_A' if robot == 'B' else \n                'ir_B')\n\n\nclass AgenteReactivoModeloDosCuartos(entornos_f.Agente):\n    \"\"\"\n    Un agente reactivo basado en modelo\n\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Inicializa el modelo interno en el peor de los casos\n\n        \"\"\"\n        self.modelo = ['A', 'sucio', 'sucio']\n\n    def programa(self, percepci\u00f3n):\n        robot, situaci\u00f3n = percepci\u00f3n\n\n        # Actualiza el modelo interno\n        self.modelo[0] = robot\n        self.modelo[' AB'.find(robot)] = situaci\u00f3n\n\n        # Decide sobre el modelo interno\n        a, b = self.modelo[1], self.modelo[2]\n        return ('nada' if a == b == 'limpio' else\n                'limpiar' if situaci\u00f3n == 'sucio' else\n                'ir_A' if robot == 'B' else 'ir_B')\n\n\ndef prueba_agente(agente):\n    entornos_f.imprime_simulacion(\n        entornos_f.simulador(\n            DosCuartos(),\n            agente,\n            [\"A\", \"sucio\", \"sucio\"],\n            100\n        ),\n        [\"A\", \"sucio\", \"sucio\"]\n    )\n\ndef test():\n    \"\"\"\n    Prueba del entorno y los agentes\n\n    \"\"\"\n    print(\"Prueba del entorno con un agente aleatorio\")\n    prueba_agente(AgenteAleatorio(['ir_A', 'ir_B', 'limpiar', 'nada']))\n\n    print(\"Prueba del entorno con un agente reactivo\")\n    prueba_agente(AgenteReactivoDoscuartos())\n\n    print(\"Prueba del entorno con un agente reactivo con modelo\")\n    prueba_agente(AgenteReactivoModeloDosCuartos())\n\n\nif __name__ == \"__main__\":\n    test()\n</code></pre>"},{"location":"#clase-20012026","title":"Clase \u2013 20/01/2026","text":""}]}